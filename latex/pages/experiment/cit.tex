\subsection{Column Image Transformer}
    
    The Column Image Transformer (CIT) is the first transformer-based model approach in this thesis that processes input data in a columnar format as examined in the previous section in more detail see \autoref{sec:IntroColumnModel}.

    %TODO: Add more information and a illustration
    This section will go more into detail about the steps form the [Data loader -> Pixel Embedding -> Positional Embedding -> transformer layer -> Pixel Emb ->  Loss calculation and evaluation].
    
    \subsubsection{Get Data as Columns}

    The data is loaded as a DataLoader object \autoref{sec:DataHandling}, which is then iterated over to obtain the data in (Batch, Color, Height, Width) format. The data is then reshaped into a columnar format, resulting in a tensor of shape (Batch, Height, Color). This reshaping is achieved by indexing the data tensor as follows:

\begin{figure}[H]
\centering
\begin{lstlisting}[language=Python]

# data: (Batch, Color, Height, Width)
def get_batch(data):

    x = data[:, :BLOCK_SIZE, :BATCH_SIZE]
    y = data[:, 1:BLOCK_SIZE+1, :BATCH_SIZE]

    x = rearrange(x, 'c h b -> b h c')
    y = rearrange(y, 'c h b -> b h c')

    return x, y
\end{lstlisting}
\caption{Python function to convert data into a columnar format}
\label{fig:get_batch_CIT}
\end{figure}

    The resulting tensor \(x\) is the input to the model, and the tensor \(y\) is the target output. The \(y\) tensor is shifted by one position to the right so that the model can learn to predict the next column based on the previous columns.

    \subsubsection{Pixel Embedding}

    The input part of the data \(x\) is embedded into a higher-dimensional space using pixel embedding layers. This MLP is a straightforward linear layer that maps the input data to a higher-dimensional space, transitioning from 3 color channels to \(N_{\text{EMBD}}\). This layer consists of a linear transformation followed by a ReLU activation function and is succeeded by a dropout layer.

    Through testing and experimentation, it was discovered that the optimal configuration for the pixel embedding layer involves a specific sequence of dimensionality increases. The most effective pathway identified begins by scaling the dimensionality of the input from the original image channels \(C\) to one-fifth of \(N_{\text{EMBD}}\) (\(\frac{1}{5}N_{\text{EMBD}}\)), then increasing it to one-half of \(N_{\text{EMBD}}\) (\(\frac{1}{2}N_{\text{EMBD}}\)) until reaching \(N_{\text{EMBD}}\). The pixel embedding layer is followed by a dropout layer with a dropout rate of 0.2.

    It is crucial to highlight the importance of excluding a layer normalization (layernorm) layer in the initial few layers of the model. Incorporating layernorm early on results in outputs that are predominantly black and white. This phenomenon occurs because the color channels \(C\), which are comprised of Red, Green, and Blue, are normalized by the layernorm layer to exhibit uniform values across these channels. As a consequence, this normalization process hinders the model's ability to learn the distinct colors of the pixels, relegating it to only wrong grayscale variations.

    \subsubsection{Positional Embedding}
    \label{sec:CIT_PositionalEmbedding}

    The positional embedding layer in this model utilizes a learnable embedding matrix of size BLOCK\_SIZE times \(N_{\text{EMBD}}\) to add positional context to the input data. This layer operates by assigning each position in the sequence a unique embedding vector, thereby mapping the original positions to a high-dimensional space of dimension \(N_{\text{EMBD}}\). The integration of positional embeddings is achieved through direct addition to the output of the pixel embedding.

\begin{align*}
    \text{pos\_emb} = \texttt{self.position\_embedding\_table}(\text{torch.arange}(\texttt{H}))
\end{align*}

The combined embedding, \texttt{x}, is calculated as:
\begin{align*}
    x = TOK_{\text{EMBD}} + POS_{\text{EMBD}}
\end{align*}

    

    \subsubsection{Classification or Regression}

    To see if the method (Classification / Regression) has a big impact on the performance of the model both methods are tested. For comparison grayscale input and outputs are used. This strategy is particularly advantageous because it minimizes the complexity arising from the vast number of potential output possibilities in classification tasks. With RGB color representation, each pixel can be classified into one of 16,581,375 distinct color combinations (255 possibilities for each of the Red, Green, and Blue channels).

    By utilizing grayscale input and outputs, we simplify the classification task, reducing the number of potential outcomes to a manageable range. Grayscale images, with their single intensity channel representing luminance, offer a more straightforward basis for analysis compared to the intricate color variations present in RGB images. This simplification allows for a more focused evaluation and a faster check if the model performs differently or better.


    \subsubsection{Sigmoid Compared to Clamp}
    \label{sec:sigmoid_vs_clamp}
    
    Due to discoloration issues in the model output, the performance is evaluated using two different activation functions: sigmoid and linear clamp. Typically, the sigmoid function maps input values to a range between 0 and 1. However, in the context of color values, this compression into the [0, 1] range can prevent achieving true black and white colors, as extremely large or small inputs are needed to reach the extremes of 0 or 1. Initially, the linear clamp function, which restricts input values to a specified range without compression, seemed to better preserve the distinct black and white colors. However, it has several notable disadvantages, such as ...(please fill in the biggest problem with backpropagation).
    
    %TODO: Add more information about the Problem with the clamp function

    In discussions about this problem, it was suggested that the lack of training data featuring significant black and white samples might be a contributing factor. After collecting more data and expanding the training set, the issue was resolved, and the model performance with the sigmoid activation function was reevaluated, showing improvement comparable to the clamp function.
    
    \subsubsection{Discoloration in the Model Output}
    
    The model faces similar discoloration issues as mentioned in section \autoref{sec:sigmoid_vs_clamp} with certain colors. When the model starts with a random pixel, it often struggles to consistently generate the intended pattern in that color or starting pattern. Some evidence suggests that the training set may be too small, as the issue was less noticeable with a larger dataset. However, this problem has not yet been resolved, and further research is needed.
        
    \subsubsection{Multi GPU Training}
    To enhance the training speed of larger models, utilizing multiple GPUs can be significantly more efficient. Consequently, the model's code has been adapted to support multi-GPU training while maintaining the original core structure. Necessary adjustments were made to the data DataLoader, logger, and trainer. For distributed training across multiple GPUs, the PyTorch library Distributed-Data-Parallel (DDP) is used. This implementation draws on guidelines from the PyTorch documentation \autocite{Subramanian2023}. The larger models are trained on an external Slurm cluster equipped with four A100 Tesla GPUs.