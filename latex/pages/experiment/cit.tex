\subsection{Column Image Transformer}
    
    The Column Image Transformer (CIT) is the first transformer-based model approach in this thesis. It processes input data in a columnar format as examined in the previous section in more detail see \autoref{sec:IntroColumnModel}.


    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{imgs/CITModel.png}
        \caption{Column Image Transformer model structure}
        \label{fig:ColumnImageTransformer}
    \end{figure}

    The image above shows the steps carried out by the CIT model. The input data is reshaped into a columnar format, which is then embedded into a higher-dimensional space. The positional embedding layer adds positional context to the input data. The transformer layer processes the data, and the output is then fed through a final layer normalization and another Image Reconstruction Layer that converts back from the embedded layer to 3 colors, the model is trained using a Mean Squared Error (MSE) loss function.

    \subsubsection{Get Data as Columns}

    The data is loaded into a DataLoader object as described in \autoref{sec:DataHandling} and iterated over to extract it as a 4-dimensional tensor (Batch, Color Channels, Height, Width). This data is reshaped into a columnar format, resulting in two tensors: the \(source\) and \(targets\), both with the shape (Batch, Height, Color Channels). The target tensor is shifted one pixel downwards to the prediction of subsequent pixels by the model. This reshaping and shifting method is commonly employed in transformer-style learning because it enables the model to process data more efficiently in terms of computational speed and resource utilization. The source tensor is then fed through the model and its final output is compared against the targets using the Mean Squared Error (MSE) loss function.

\begin{figure}[H]
\centering
\begin{lstlisting}[language=Python]
# data: (Batch, Color, Height, Width)
def get_batch(data):

    source = data[:, :BLOCK_SIZE, :BATCH_SIZE]
    targets = data[:, 1:BLOCK_SIZE+1, :BATCH_SIZE]

    source = rearrange(source, 'c h b -> b h c')
    targets = rearrange(targets, 'c h b -> b h c')

    return source, targets
\end{lstlisting}
\caption{Python function to convert data into a columnar format}
\label{fig:get_batch_CIT}
\end{figure}

    \subsubsection{Pixel Embedding}

    The \(source\) data is embedded into a higher-dimensional space using pixel embedding layers. This multilayer perceptron (MLP) is a straightforward linear layer that maps the input data to a higher-dimensional space, transitioning from \(3\) color channels to \(N_{\text{EMBD}}\). These layers consist of a linear transformation followed by a ReLU activation function and are succeeded by a dropout layer.

    Through testing and experimentation, it seems that the optimal configuration for the pixel embedding layer involves a sequence of dimensionality increases. The most effective pathway identified begins by scaling the dimensionality of the input from the original image channels \(C\) to one-fifth of \(N_{\text{EMBD}}\) (\(\frac{1}{5}N_{\text{EMBD}}\)), then increasing it to one-half of \(N_{\text{EMBD}}\) (\(\frac{1}{2}N_{\text{EMBD}}\)) until reaching \(N_{\text{EMBD}}\). The pixel embedding layer is followed by a dropout layer with a dropout rate of 0.2. See \autoref{fig:posEmbend_CIT}.

    It is crucial to highlight the importance of not using a layer normalization layernorm layer in the initial few layers of the model. Incorporating a layernorm layer too early results in outputs that are predominantly black and white. This phenomenon occurs because the color channels \(C\), which are comprised of red, green, and blue, are normalized by the layernorm layer to exhibit uniform values across these channels. As a consequence, this normalization process hinders the model's ability to learn the distinct colors of the pixels, relegating it to only wrong grayscale variations.

    \subsubsection{Positional Embedding}
    \label{sec:CIT_PositionalEmbedding}

    The positional embedding layer in this model utilizes a learnable embedding matrix of size \text{block\_size} times \(N_{\text{EMBD}}\), where \text{block\_size} represents the context length, or the height of an image. This layer operates by assigning each position within this vertical context a unique embedding vector. Thereby mapping the original pixel positions to a high-dimensional space characterized by \(N_{\text{EMBD}}\) dimensions. The integration of these positional embeddings is achieved through their direct addition to the output of the pixel embeddings, effectively enhancing the model's ability to maintain positional awareness across the image height.


\begin{figure}[H]
    \centering
    \begin{lstlisting}[language=Python]
class ColumnTransformer(nn.Module):
    self.pixel_embedding = nn.Sequential(
        nn.Linear(CHANNELS_IMG, N_EMBD//5, device = device),
        nn.ReLU(),
        nn.Linear(N_EMBD//5, N_EMBD//2, device = device),
        nn.ReLU(),
        nn.Linear(N_EMBD//2, N_EMBD, device = device),
        nn.Dropout(DROPOUT),
    )
    self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)
    #[...]

def forward(self, source, targets=None):

    # tok_emb: (B, H, N_EMBD)
    tok_emb = self.pixel_embedding(source) 

    # pos_emb: (H, N_EMBD)
    pos_emb = self.position_embedding_table(torch.arange(H)) 

    # x: (B,H,N_EMBD)
    x = tok_emb + pos_emb 
    #[...]
\end{lstlisting}
\caption{Python code snippet for the positional / pixel embedding layers}
\label{fig:posEmbend_CIT}
\end{figure}
   
    \subsubsection{Transformer Layer}
    \label{sec:transformer_CIT}

    The transformer layer forms the core of the model's architecture and is crucial for processing the data. It utilizes self-attention mechanisms \autocite{vaswani2023attention} to weigh the importance of different pixels relative to each other, allowing the model to focus on relevant parts of the input when predicting the next pixel in a column. 

    In this model, each transformer layer receives the input from the previous layer. The layer then processes this input using a multi-head attention mechanism, which allows the model to capture various aspects of the input at different positions simultaneously. This is followed by a series of normalization and feed-forward layers. All are combined into a block that is repeated multiple times. See \autoref{sec:transformer_layer_Python} for the code snippet.


    \subsubsection{Image Reconstruction Layer}

    The image reconstruction layer is like the pixel embedding a simple MLP that converts the data back to the original color space. It has the same structure but reversed like the Pixel Embedding layer but with the output dimensionality set to \(3\), representing the red, green, and blue color channels. After the shrinkage of the dimensionality, the output is passed through one final sigmoid to map the resulting colors to 0-1.

    \subsubsection{Sigmoid Compared to Clamp}
    \label{sec:sigmoid_vs_clamp}
    
    Due to discoloration issues in the model output, the performance has been evaluated using two different activation functions: sigmoid and linear clamp. The sigmoid function typically maps input values to a range between 0 and 1. However, in the context of color values, this compression into the [0, 1] range can prevent achieving true black and white colors, as inputs need to be extremely large or small to reach the extremes of 0 or 1. Initially, the linear clamp function, which restricts input values to a specified range without compression, seemed to better preserve the distinct black and white colors. However, it has several notable disadvantages, such as the issue of vanishing gradients during backpropagation. A problem where gradients become so small during backpropagation that the model stops learning. This occurs because the derivative of the clamp function outside its bounds is zero, which stops the gradient flow and prevents weights from updating. Consequently, layers deeper in the network receive minimal or no updates, hindering the models training process.
    
    In discussions about this problem, it was suggested that the lack of training data featuring significant black and white samples might be a contributing factor. After collecting more data and expanding the training set, the issue was resolved, and the model performance with the sigmoid activation function became very similar, showing improvement comparable to the clamp function.
    
    \subsubsection{Discoloration in the Model Output}
    
    The model faces similar discoloration issues as mentioned in section \autoref{sec:sigmoid_vs_clamp} with certain colors. When the model starts with a random pixel, it often struggles to consistently generate the intended pattern in that color or starting pattern. Some evidence suggests that the training set may be too small, as the issue was less noticeable with a larger dataset. Generally, transformers require significantly more data to improve performance \autocite{chen2022dearkd}.

    \subsubsection{Training the Model}

    The training process starts by selecting the hyperparameters and arguments for the model. To test the basic functionality of the model, the training process is kept simple, and they are trained on a single GPU. Most of the training is done via the Slurm cluster, which uses a batch script to configure the hardware and training parameters. All the relevant trained models [model 0-4] with their corresponding parameters can be found in detail in the appendix, see \autoref{sec:trained_models_hyperparameters}.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/Training_Metrics_CIT 2.1.8.1_512.png}
        \caption{Loss plot of the CIT [model 4]}
        \label{fig:Training_Metrics_CIT512}
    \end{figure}

    The figure above displays the training metrics for the Column Image Transformer [model 4] CIT 2.1.8.1\_512, illustrating the loss over iterations during the training phase. The blue line signifies the training validation loss, and the orange line represents the validation loss throughout the training iterations. The graph shows a consistent decline in both metrics, indicating that the model is learning and improving its predictive accuracy over time. The 'Cutoff Point,' indicated by the dashed red line, marks the iteration at which the model is used in the examples presented in this thesis since it has not shown further improvement beyond this point. An unusual behavior observed is that the validation loss is lower than the training validation loss, which is uncommon. This could be the case due to various factors, such as a high dropout rate and needs further investigation.

    \subsubsection{Experimental Results}

    Multiple experiments are conducted to evaluate the performance of the CIT model. These experiments are designed to test the model's ability to generate images, the quality of the generated images and the model's performance across different scenarios. The results are analyzed to identify the strengths and weaknesses of the model.
    
    In each test, the end of the seed pixels are marked with a purple line on the left side of the image. All trained models [0-4] can be found in the appendix, see \autoref{sec:trained_models_hyperparameters}.
    
    \begin{itemize}
        \item \textbf{Color test:} To assess the model's capability to generate specific colors, it is challenged with 11 different images with a width of 32 and a height of 128 pixels. Each test image is composed of a single color filling 128 positions in the model's context. The model then generates the next 32 pixels for each image.
    
        \begin{figure}[H]
            \centering
            % First row
            \begin{minipage}{0.45\textwidth}
                \centering
                \includegraphics[width=\textwidth]{imgs/ColorTest_2.1.7.1_OldData.png} 
                \subcaption{[model 0] 2.1.7.1}
                \label{fig:test1_M0_Cit}
            \end{minipage}
            \hfill
            \begin{minipage}{0.45\textwidth}
                \centering
                \includegraphics[width=\textwidth]{imgs/ColorTest_2.1.8.1_128.png} 
                \subcaption{[model 1] 2.1.8.1 [128 Context]}
                \label{fig:test1_M1_Cit}
            \end{minipage}
    
            % Second row
            \begin{minipage}{0.45\textwidth}
                \centering
                \includegraphics[width=\textwidth]{imgs/ColorTest_2.1.8.1_256.png} 
                \subcaption{[model 2] 2.1.8.1 [256 Context]}
                \label{fig:test1_M2_Cit}
            \end{minipage}
            \hfill
            \begin{minipage}{0.45\textwidth}
                \centering
                \includegraphics[width=\textwidth]{imgs/ColorTest_2.1.8.1_512.png} 
                \subcaption{[model 4] 2.1.8.1 [512 Context]}
                \label{fig:test1_M3_Cit}
            \end{minipage}
            \caption{Color test with four different models}
        \end{figure}
    
        As shown in the images above, model [0], which uses the old small dataset, struggles to generate the correct colors due to the limited training data compared to Models [1-3]. Model [1] generally produces accurate colors but faces difficulties with some. Model [2] generates all colors correctly, but inconsistencies start appearing in column 8. The model may begin changing colors due to its training to recognize patterns such as flowers or carpets in the training data. Model [4] consistently generates the correct colors but begins to create patterns in some columns, maybe indicating that the model is overfitting.
 
        \item \textbf{Zebra pattern test:} The second test assesses whether the model can generate a zebra pattern. The model is provided with both a vertical and a horizontal test image featuring black and white stripes, each stripe being 4 pixels wide. It then generates the next 32 pixels. 

        \begin{figure}[H]
            \centering
            % First row
            \begin{minipage}{0.45\textwidth}
                \centering
                \includegraphics[width=0.5\textwidth]{imgs/ZebraTestV_2.1.7.1_OldData.png} 
                \subcaption{[model 0] 2.1.7.1}
                \label{fig:test2_0_M0_Cit}
            \end{minipage}
            \hfill
            \begin{minipage}{0.45\textwidth}
                \centering
                \includegraphics[width=0.5\textwidth]{imgs/ZebraTestV_2.1.8.1_512.png} 
                \subcaption{[model 4] 2.1.8.1 [512 Context]}
                \label{fig:test2_0_M5_Cit}
            \end{minipage}
            \caption{Zebra test vertical with two different models}
        \end{figure}

        As shown in the images above, the model with the newer dataset model[4] generates the colors in the zebra pattern more accurately than the model with the older dataset model[0]. Models [0-2] perform similarly. The vertical test is particularly interesting to observe when the column model is altered to see pixels from both the left and the right sides of the current column, assessing whether it can still perfectly continue generating the pattern.


        \begin{figure}[H]
            \centering
            % Second row
            \begin{minipage}{0.45\textwidth}
                \centering
                \includegraphics[width=0.5\textwidth]{imgs/ZebraTestH_2.1.7.1_OldData.png} 
                \subcaption{[model 1] 2.1.8.1}
                \label{fig:test2_1_M0_Cit}
            \end{minipage}
            \hfill
            \begin{minipage}{0.45\textwidth}
                \centering
                \includegraphics[width=0.5\textwidth]{imgs/ZebraTestH_2.1.8.1_512.png} 
                \subcaption{[model 4] 2.1.8.1 [512 Context]}
                \label{fig:test2_1_M5_Cit}
            \end{minipage}
            \caption{Zebra test horizontal with two different models}
        \end{figure}

        Unfortunately, the model struggles to generate the zebra pattern horizontally. The model with the newer dataset model [4] more accurately renders the colors in the zebra pattern than the model with the older dataset model [0], but both models struggle to correctly continue the pattern. Model [4] can recognize that the next pixel in the pattern should be dark but quickly loses context. This could be due to the dataset being too small or the model size being insufficient to fully understand the pattern. The images suggest that model [4] is better than model [1] but still fails to generate the pattern correctly.
       
    \end{itemize}

    \subsubsection{Challenges and Limitations}

    The CIT model has some limitations and challenges that require attention. The primary issue is its limited context to only one column, which hampers its ability to generate complex patterns requiring a broader visual spectrum. Additionally, similar to some text-based Transformer models, the CIT model struggles with generating coherent long sequences, often using its recent outputs as a basis for predicting subsequent pixels. This can lead to errors accumulating over time, particularly evident in longer image sequences where the model fails to maintain the specific pattern. 

    To address these limitations, methods used in text-based models, such as scaling up the model size and expanding the training dataset, can be considered. Scaling the model could involve increasing the dimensionality of the embeddings or adding more layers, which might help the model capture more complex dependencies. However, expanding the dataset was not feasible in this thesis due to time constraints and the extensive duration of the dataset collection process.

    Introducing a text prompt feature could significantly enhance the usability and flexibility of the CIT model. This feature would allow users to guide the generation process using descriptive language, making it easier to specify desired outcomes without relying solely on seed pixels. Such a capability would not only improve user interaction but could also potentially help the model deal with complex pattern generation by providing additional contextual information. Exploring this possibility could be a valuable direction for future research.
