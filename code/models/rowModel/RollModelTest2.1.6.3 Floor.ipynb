{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "BLOCK_SIZE: 128, BATCH_SIZE: 128, CHANNELS_IMG: 3, IMAGE_SIZE: 141, N_EMBD: 128\n",
      "Model has 1,217,867 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import ConcatDataset\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 128\n",
    "BLOCK_SIZE = 128\n",
    "IMAGE_SIZE = 129 + 12\n",
    "CHANNELS_IMG = 3\n",
    "\n",
    "N_EMBD = 128\n",
    "N_HEAD = 6\n",
    "N_LAYER = 6\n",
    "DROPOUT = 0.2\n",
    "\n",
    "VERSION = \"2.1.6.3_bigData\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def model_saveFile(version, train):\n",
    "    return f\"model/model{train}{VERSION}_FloorEP{version}.pth\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \",device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader):\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for idx, (data, _) in enumerate(dataloader):\n",
    "        dataRaw = data.squeeze(0).to(device)\n",
    "        x, y = get_batch(dataRaw)\n",
    "        \n",
    "        _, loss = model(x, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_samples += 1\n",
    "    return total_loss / total_samples\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.query = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.value = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, N_EMBD)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(DROPOUT),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "class ColumnTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pixel_embedding = nn.Sequential(\n",
    "            nn.Linear(CHANNELS_IMG, N_EMBD//5, device = device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_EMBD//5, N_EMBD//2, device = device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_EMBD//2, N_EMBD, device = device),\n",
    "            nn.Dropout(DROPOUT),\n",
    "        )\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
    "        self.blocks = nn.Sequential(*[Block(N_EMBD, n_head=N_HEAD) for _ in range(N_LAYER)])\n",
    "        self.ln_f = nn.LayerNorm(N_EMBD) # final layer norm\n",
    "        self.lm_head = nn.Sequential(\n",
    "            nn.Linear(N_EMBD, N_EMBD//2, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_EMBD//2, N_EMBD//5, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_EMBD//5, CHANNELS_IMG, device=device),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, H, C = idx.shape\n",
    "        \n",
    "        tok_emb = self.pixel_embedding(idx) # (B, H, N_EMBD)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(H, device=device)) # (H, N_EMBD)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,H,N_EMBD)\n",
    "\n",
    "\n",
    "        x = self.blocks(x) # (B,H,N_EMBD)\n",
    "        x = self.ln_f(x) # (B,H,N_EMBD)\n",
    "        logits = self.lm_head(x) # (B,H,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # B, T, C = logits.shape\n",
    "            loss = F.mse_loss(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(IMAGE_SIZE),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f'BLOCK_SIZE: {BLOCK_SIZE}, BATCH_SIZE: {BATCH_SIZE}, CHANNELS_IMG: {CHANNELS_IMG}, IMAGE_SIZE: {IMAGE_SIZE}, N_EMBD: {N_EMBD}')\n",
    "\n",
    "m = ColumnTransformer()\n",
    "m = m.to(device)\n",
    "\n",
    "#print parameterscount\n",
    "print(f'Model has {count_parameters(m):,} trainable parameters')\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_batch(data):\n",
    "    # C ,H ,W = data.shape\n",
    "\n",
    "    x = data[:, :BLOCK_SIZE, :BATCH_SIZE]\n",
    "    y = data[:, 1:BLOCK_SIZE*2+1, :BATCH_SIZE]\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    x = rearrange(x, 'c h b -> b h c')\n",
    "    y = rearrange(y, 'c h b -> b h c')\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "m = torch.load(f\"tempModel/modelpretrain2.1.7.1_AllDataEP0.pth\").to(device)\n",
    "\n",
    "def create_striped_tensor(width, height, stripe_width):\n",
    "    tensor = torch.zeros(1, 3, height, width, dtype=torch.uint8, device=device)\n",
    "\n",
    "    # Create stripes\n",
    "    for row in range(height):\n",
    "        if (row // stripe_width) % 2 != 0:\n",
    "            tensor[:, :, row, :] = 1  # White stripe\n",
    "\n",
    "    return tensor\n",
    "\n",
    "    \n",
    "\n",
    "with torch.no_grad():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"device: \",device)\n",
    "\n",
    "\n",
    "    zebra_tensor = rearrange(create_striped_tensor(width=64, height=64, stripe_width=2), '1 c h w -> 1 c h w')\n",
    "\n",
    "    # Displaying the tensor as an image\n",
    "    plt.imshow((rearrange(zebra_tensor, '1 c h w -> h w c')* 255).cpu().detach().numpy().astype('int'))\n",
    "    plt.axis('off')  # Turn off axis numbers\n",
    "    plt.show()\n",
    "\n",
    "    size_x = 128\n",
    "    size_y = 128\n",
    "\n",
    "    gen_length = 32\n",
    "\n",
    "\n",
    "    img_tensor = zebra_tensor.squeeze(0).to(device)\n",
    "    img_tensor = img_tensor[:3,:,:] # Remove alpha channel\n",
    "    \n",
    "    C, H, W = img_tensor.shape\n",
    "\n",
    "    img_gen = torch.zeros((C,H+gen_length,W)).to(device)\n",
    "    img_gen[:,:H,:] = img_tensor\n",
    "\n",
    "\n",
    "    for igen in range(gen_length):\n",
    "        \n",
    "        a = img_gen[:,igen:H+igen,:] # (C, H, W) \n",
    "        gen, _ = m(rearrange(a, 'c h w -> w h c')) # B, H, C \n",
    "\n",
    "        img_gen[:,H+igen,:] = rearrange(gen[:,-1,:], ' w c -> c w')\n",
    "\n",
    "    \n",
    "    img_gen = img_gen.squeeze(0) * 255\n",
    "    \n",
    "    image = rearrange(img_gen, 'c h w -> h w c').cpu().detach().numpy()\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    image[H-1, :6] = [255, 100, 255]\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFB0lEQVR4nO3bMQrDMBQFQSnk/lf+6ba2CNiBzNTioW5RoT0zswBgrfV6+gIA/A5RACCiAEBEAYCIAgARBQAiCgBEFADI++rBvffR8MmfONv3bp/u2/5+3/a926f7/7J9hZcCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgDZMzNPXwKA3+ClAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAPiBfOwOhpVrIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "m = torch.load(f\"tempModel/modelpretrain2.1.7.1_AllDataEP0.pth\").to(device)\n",
    "\n",
    "def create_striped_tensor(width, height, stripe_width):\n",
    "    tensor = torch.zeros(1, 3, height, width, dtype=torch.uint8, device=device)\n",
    "\n",
    "    # Create stripes\n",
    "    for row in range(height):\n",
    "        if (row // stripe_width) % 2 != 0:\n",
    "            tensor[:, :, row, :] = 1  # White stripe\n",
    "\n",
    "    return tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"device: \",device)\n",
    "\n",
    "\n",
    "    zebra_tensor = rearrange(create_striped_tensor(width=64, height=64, stripe_width=2), '1 c h w -> 1 c w h')\n",
    "\n",
    "    # Displaying the tensor as an image\n",
    "    plt.imshow((rearrange(zebra_tensor, '1 c h w -> h w c')* 255).cpu().detach().numpy().astype('int'))\n",
    "    plt.axis('off')  # Turn off axis numbers\n",
    "    plt.show()\n",
    "\n",
    "    size_x = 128\n",
    "    size_y = 128\n",
    "\n",
    "    gen_length = 32\n",
    "\n",
    "\n",
    "    img_tensor = zebra_tensor.squeeze(0).to(device)\n",
    "    img_tensor = img_tensor[:3,:,:] # Remove alpha channel\n",
    "    \n",
    "    C, H, W = img_tensor.shape\n",
    "\n",
    "    img_gen = torch.zeros((C,H+gen_length,W)).to(device)\n",
    "    img_gen[:,:H,:] = img_tensor\n",
    "\n",
    "\n",
    "    for igen in range(gen_length):\n",
    "        \n",
    "        a = img_gen[:,igen:H+igen,:] # (C, H, W) \n",
    "        gen, _ = m(rearrange(a, 'c h w -> w h c')) # B, H, C \n",
    "\n",
    "        img_gen[:,H+igen,:] = rearrange(gen[:,-1,:], ' w c -> c w')\n",
    "\n",
    "    \n",
    "    img_gen = img_gen.squeeze(0) * 255\n",
    "    \n",
    "    image = rearrange(img_gen, 'c h w -> h w c').cpu().detach().numpy()\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    image[H-1, :6] = [255, 100, 255]\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
