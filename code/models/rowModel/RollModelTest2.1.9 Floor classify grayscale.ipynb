{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "BLOCK_SIZE: 64, BATCH_SIZE: 64, CHANNELS_IMG: 1, IMAGE_SIZE: 65, N_EMBD: 128\n",
      "Model has 1,255,424 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import ConcatDataset\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 64\n",
    "BLOCK_SIZE = 64\n",
    "IMAGE_SIZE = 65\n",
    "CHANNELS_IMG = 1\n",
    "\n",
    "N_EMBD = 128\n",
    "N_HEAD = 6\n",
    "N_LAYER = 6\n",
    "DROPOUT = 0.2\n",
    "\n",
    "VERSION = \"2.1.9.1_bigData_qHD\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def model_saveFile(version, train):\n",
    "    return f\"model/model{train}{VERSION}_FloorEP{version}.pth\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "print(\"device: \",device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader):\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for idx, (data, _) in enumerate(dataloader):\n",
    "        dataRaw = data.squeeze(0).to(device)\n",
    "        x, y = get_batch(dataRaw)\n",
    "        \n",
    "        _, loss = model(x, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_samples += 1\n",
    "    return total_loss / total_samples\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.query = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.value = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, N_EMBD)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(DROPOUT),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "class ColumnTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(256, N_EMBD)\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
    "        self.blocks = nn.Sequential(*[Block(N_EMBD, n_head=N_HEAD) for _ in range(N_LAYER)])\n",
    "        self.ln_f = nn.LayerNorm(N_EMBD) # final layer norm\n",
    "        self.lm_head = nn.Linear(N_EMBD, 256, device = device)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, H = idx.shape\n",
    "\n",
    "        idx *= 255\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx.long()) # (B, H, N_EMBD)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(H, device=device)) # (H, N_EMBD)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,H,N_EMBD)\n",
    "\n",
    "        x = self.blocks(x) # (B,H,N_EMBD)\n",
    "        x = self.ln_f(x) # (B,H,N_EMBD)\n",
    "\n",
    "        logits = self.lm_head(x) # (B,H,256)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits_BH = logits.view(B*H, 256)\n",
    "            targets = targets.reshape(B*H) * 255\n",
    "\n",
    "            loss = F.cross_entropy(logits_BH, targets.long())\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "print(f'BLOCK_SIZE: {BLOCK_SIZE}, BATCH_SIZE: {BATCH_SIZE}, CHANNELS_IMG: {CHANNELS_IMG}, IMAGE_SIZE: {IMAGE_SIZE}, N_EMBD: {N_EMBD}')\n",
    "\n",
    "\n",
    "m = ColumnTransformer()\n",
    "m = m.to(device)\n",
    "\n",
    "#print parameterscount\n",
    "print(f'Model has {count_parameters(m):,} trainable parameters')\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_batch(data):\n",
    "    # C ,H ,W = data.shape\n",
    "\n",
    "    x = data[0,:BLOCK_SIZE, :BATCH_SIZE].clone()\n",
    "    y = data[0,1:BLOCK_SIZE+1, :BATCH_SIZE].clone()\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    x = rearrange(x, 'h b -> b h')\n",
    "    y = rearrange(y, 'h b -> b h')\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_batch(data, gen_length):\n",
    "    # C ,H ,W = data.shape\n",
    "\n",
    "    x = data[:BLOCK_SIZE, :BATCH_SIZE].clone()\n",
    "    y = data[1:BLOCK_SIZE+1 + gen_length, :BATCH_SIZE].clone()\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    x = rearrange(x, 'h b -> b h')\n",
    "    y = rearrange(y, 'h b -> b h')\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_striped_tensor(width, height, stripe_width):\n",
    "    tensor = torch.zeros(1, height, width, dtype=torch.uint8, device=device)\n",
    "\n",
    "    # Create stripes\n",
    "    for row in range(height):\n",
    "        if (row // stripe_width) % 2 != 0:\n",
    "            tensor[:, row, :] = 1  # White stripe\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# torch.manual_seed(1337)\n",
    "with torch.no_grad():\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = 'cpu'\n",
    "    print(\"device: \",device)\n",
    "\n",
    "\n",
    "    m = torch.load(\"tempModel/modeltest2.1.9.1_bigData_qHD_FloorEP0.pth\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "    size_x = 64\n",
    "    size_y = 64\n",
    "\n",
    "    gen_length = 32\n",
    "\n",
    "\n",
    "    zebra_tensor = create_striped_tensor(width=64, height=64, stripe_width=2)\n",
    "    img_tensor = rearrange(zebra_tensor, '1 h w -> w h').to(device)\n",
    "    \n",
    "    H, W = img_tensor.shape\n",
    "\n",
    "    x, y = get_batch(img_tensor, gen_length) # (B,H)\n",
    "\n",
    "\n",
    "    img_gen = torch.zeros((W, H+gen_length)).to(device)\n",
    "    img_gen[:,:H] = x\n",
    "\n",
    "    for igen in range(gen_length):\n",
    "\n",
    "        a = img_gen[:,igen:H+igen] # (W, H) \n",
    "\n",
    "        gen, _ = m(a.clone()) # B, H\n",
    "\n",
    "        softmaxed = F.softmax(gen[:,-1], dim=-1)\n",
    "\n",
    "        # Anwenden von Multinomial auf jede Farbe\n",
    "        r_samples_flat = torch.multinomial(softmaxed, 1).squeeze(-1) / 255.0\n",
    "\n",
    "        img_gen[:,H+igen] = r_samples_flat\n",
    "\n",
    "    # Konvertierung in ein numpy-Array und Normalisierung\n",
    "    image_np = img_gen.t().cpu().detach().numpy().astype(float) / 255.0\n",
    "    target_np = y.t().cpu().detach().numpy().astype(float)\n",
    "\n",
    "    plt.subplot(1, 2, 1)  # 1 Reihe, 2 Spalten, Position 1\n",
    "    plt.imshow(target_np, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)  # 1 Reihe, 2 Spalten, Position 2\n",
    "    plt.imshow(image_np, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
