{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spiral matrix\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "def create_spiral(n):\n",
    "    # Initialize a n x n matrix\n",
    "    matrix = [[0] * n for _ in range(n)]\n",
    "\n",
    "    x, y = 0, 0\n",
    "\n",
    "    # Direction vectors (right, down, left, up)\n",
    "    dx = [0, 1, 0, -1]\n",
    "    dy = [1, 0, -1, 0]\n",
    "    direction = 0\n",
    "\n",
    "    for i in range(n * n - 1, -1, -1):  # Start from n*n - 1 (35 for 6x6) and go down to 0\n",
    "        matrix[x][y] = i\n",
    "\n",
    "        nx = x + dx[direction]\n",
    "        ny = y + dy[direction]\n",
    "\n",
    "        # Change direction if next position is out of bounds or already filled\n",
    "        if nx < 0 or nx >= n or ny < 0 or ny >= n or matrix[nx][ny] != 0:\n",
    "            direction = (direction + 1) % 4  # Change direction\n",
    "            nx = x + dx[direction]\n",
    "            ny = y + dy[direction]\n",
    "\n",
    "        x, y = nx, ny\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[63, 62, 61, 60, 59, 58, 57, 56],\n",
       " [36, 35, 34, 33, 32, 31, 30, 55],\n",
       " [37, 16, 15, 14, 13, 12, 29, 54],\n",
       " [38, 17, 4, 3, 2, 11, 28, 53],\n",
       " [39, 18, 5, 0, 1, 10, 27, 52],\n",
       " [40, 19, 6, 7, 8, 9, 26, 51],\n",
       " [41, 20, 21, 22, 23, 24, 25, 50],\n",
       " [42, 43, 44, 45, 46, 47, 48, 49]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spiral_matrix = create_spiral(8)\n",
    "spiral_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[63, 62, 61, 60, 59, 58, 57, 56],\n",
      "        [36, 35, 34, 33, 32, 31, 30, 55],\n",
      "        [37, 16, 15, 14, 13, 12, 29, 54],\n",
      "        [38, 17,  4,  3,  2, 11, 28, 53],\n",
      "        [39, 18,  5,  0,  1, 10, 27, 52],\n",
      "        [40, 19,  6,  7,  8,  9, 26, 51],\n",
      "        [41, 20, 21, 22, 23, 24, 25, 50],\n",
      "        [42, 43, 44, 45, 46, 47, 48, 49]])\n",
      "tensor([[64, 68,  0, 60,  0,  6, 62, 48],\n",
      "        [49, 19,  1, 49, 85, 53,  7, 26],\n",
      "        [28,  6, 64, 61, 92, 73, 37, 43],\n",
      "        [94, 82, 53,  1, 68, 50, 62, 62],\n",
      "        [43, 47, 36,  7, 84,  9, 94, 77],\n",
      "        [73,  3, 65, 49,  2, 42, 54, 33],\n",
      "        [41, 74, 74, 50, 91,  4, 60, 66],\n",
      "        [53, 57, 49, 39, 12, 17, 95, 89]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 7, 84, 68,  1, 53, 36, 65, 49,  2, 42,  9, 50, 73, 92, 61, 64,  6, 82,\n",
       "        47,  3, 74, 74, 50, 91,  4, 60, 54, 94, 62, 37,  7, 53, 85, 49,  1, 19,\n",
       "        49, 28, 94, 43, 73, 41, 53, 57, 49, 39, 12, 17, 95, 89, 66, 33, 77, 62,\n",
       "        43, 26, 48, 62,  6,  0, 60,  0, 68, 64])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the spiral matrix to reorder the data tensor\n",
    "\n",
    "data_tensor = torch.randint(0, 99, (8, 8))\n",
    "\n",
    "spiral_indices = torch.tensor([\n",
    "    [63, 62, 61, 60, 59, 58, 57, 56],\n",
    "    [36, 35, 34, 33, 32, 31, 30, 55],\n",
    "    [37, 16, 15, 14, 13, 12, 29, 54],\n",
    "    [38, 17,  4,  3,  2, 11, 28, 53],\n",
    "    [39, 18,  5,  0,  1, 10, 27, 52],\n",
    "    [40, 19,  6,  7,  8,  9, 26, 51],\n",
    "    [41, 20, 21, 22, 23, 24, 25, 50],\n",
    "    [42, 43, 44, 45, 46, 47, 48, 49]\n",
    "])\n",
    "\n",
    "\n",
    "flattened_data = data_tensor.flatten()\n",
    "\n",
    "correct_reordered_tensor = torch.zeros_like(flattened_data)\n",
    "\n",
    "correct_reordered_tensor[spiral_indices.flatten()] = flattened_data\n",
    "\n",
    "correct_reordered_tensor = correct_reordered_tensor.reshape(8, 8).flatten()\n",
    "print(spiral_indices)\n",
    "print(data_tensor)\n",
    "correct_reordered_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[48, 47, 46, 45, 44, 43, 42],\n",
      "        [25, 24, 23, 22, 21, 20, 41],\n",
      "        [26,  9,  8,  7,  6, 19, 40],\n",
      "        [27, 10,  1,  0,  5, 18, 39],\n",
      "        [28, 11,  2,  3,  4, 17, 38],\n",
      "        [29, 12, 13, 14, 15, 16, 37],\n",
      "        [30, 31, 32, 33, 34, 35, 36]])\n",
      "tensor([[50, 58, 32, 62,  2,  5, 14],\n",
      "        [78, 93, 33, 34, 73, 41, 84],\n",
      "        [27, 37, 63, 92,  7,  2, 42],\n",
      "        [63, 97, 48, 33, 71, 78, 38],\n",
      "        [88, 71, 20, 66, 36, 56, 66],\n",
      "        [30, 69, 79, 79, 37, 40, 79],\n",
      "        [71, 85, 12, 74, 65, 55, 53]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([33, 48, 20, 66, 36, 71,  7, 92, 63, 37, 97, 71, 69, 79, 79, 37, 40, 56,\n",
       "        78,  2, 41, 73, 34, 33, 93, 78, 27, 63, 88, 30, 71, 85, 12, 74, 65, 55,\n",
       "        53, 79, 66, 38, 42, 84, 14,  5,  2, 62, 32, 58, 50])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the spiral matrix to reorder the data tensor more efficiently\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "size = 7\n",
    "\n",
    "spiral_indices = torch.tensor(create_spiral(7))\n",
    "spiral_indices\n",
    "\n",
    "data_tensor = torch.randint(0, 99, (size,size))\n",
    "\n",
    "correct_reordered_tensor = torch.zeros_like(data_tensor.flatten())\n",
    "\n",
    "correct_reordered_tensor[spiral_indices.flatten()] = data_tensor.flatten()\n",
    "\n",
    "print(spiral_indices)\n",
    "print(data_tensor)\n",
    "correct_reordered_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3])\n",
      "tensor([[[0.3490, 0.4118, 0.3412],\n",
      "         [0.2980, 0.3020, 0.3333],\n",
      "         [0.2784, 0.2863, 0.3725]],\n",
      "\n",
      "        [[0.3451, 0.4078, 0.3373],\n",
      "         [0.2941, 0.2980, 0.3294],\n",
      "         [0.2667, 0.2745, 0.3608]],\n",
      "\n",
      "        [[0.2745, 0.3373, 0.2667],\n",
      "         [0.2235, 0.2275, 0.2588],\n",
      "         [0.2078, 0.2157, 0.2941]]])\n",
      "torch.Size([3, 9])\n",
      "tensor([[8, 7, 6],\n",
      "        [1, 0, 5],\n",
      "        [2, 3, 4]])\n",
      "torch.Size([3, 9])\n",
      "tensor([[0.3020, 0.2980, 0.2784, 0.2863, 0.3725, 0.3333, 0.3412, 0.4118, 0.3490],\n",
      "        [0.2980, 0.2941, 0.2667, 0.2745, 0.3608, 0.3294, 0.3373, 0.4078, 0.3451],\n",
      "        [0.2275, 0.2235, 0.2078, 0.2157, 0.2941, 0.2588, 0.2667, 0.3373, 0.2745]])\n",
      "x torch.Size([8, 3])\n",
      "tensor([[0.3020, 0.2980, 0.2275],\n",
      "        [0.2980, 0.2941, 0.2235],\n",
      "        [0.2784, 0.2667, 0.2078],\n",
      "        [0.2863, 0.2745, 0.2157],\n",
      "        [0.3725, 0.3608, 0.2941],\n",
      "        [0.3333, 0.3294, 0.2588],\n",
      "        [0.3412, 0.3373, 0.2667],\n",
      "        [0.4118, 0.4078, 0.3373]])\n",
      "y torch.Size([8, 3])\n",
      "tensor([[0.2980, 0.2941, 0.2235],\n",
      "        [0.2784, 0.2667, 0.2078],\n",
      "        [0.2863, 0.2745, 0.2157],\n",
      "        [0.3725, 0.3608, 0.2941],\n",
      "        [0.3333, 0.3294, 0.2588],\n",
      "        [0.3412, 0.3373, 0.2667],\n",
      "        [0.4118, 0.4078, 0.3373],\n",
      "        [0.3490, 0.3451, 0.2745]])\n"
     ]
    }
   ],
   "source": [
    "# Use the spiral matrix to getData from an image\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "IMAGE_SIZE = 3\n",
    "BLOCK_SIZE = IMAGE_SIZE * IMAGE_SIZE - 1\n",
    "BATCH_SIZE = 0\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "spiral_indices = torch.tensor(create_spiral(IMAGE_SIZE))\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_data(data):\n",
    "    C ,H ,W = data.shape\n",
    "\n",
    "    print(data.shape)\n",
    "    print(data)\n",
    "\n",
    "    spiral_data = torch.zeros_like(data.view(C, -1))\n",
    "    print(spiral_data.shape)\n",
    "\n",
    "    spiral_data[:,spiral_indices.flatten()] = data.view(C, -1)\n",
    "\n",
    "    print(spiral_indices)\n",
    "\n",
    "    print(spiral_data.shape)\n",
    "    print(spiral_data)\n",
    "\n",
    "\n",
    "    x = spiral_data[:, :BLOCK_SIZE]\n",
    "    y = spiral_data[:, 1:BLOCK_SIZE+1]\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    x = rearrange(x, 'c h -> h c')\n",
    "    y = rearrange(y, 'c h -> h c')\n",
    "    return x, y\n",
    "\n",
    "\n",
    "with Image.open(\"../resources/genImges/genApeTestPic.jpg\") as img:\n",
    "    img = transform(img)\n",
    "    dataRaw = img[:3].to(device)\n",
    "\n",
    "    x, y = get_data(dataRaw)\n",
    "\n",
    "    print(\"x\", x.shape)\n",
    "    print(x)\n",
    "    print(\"y\", y.shape)\n",
    "    print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24],\n",
      "        [25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34]])\n",
      "tensor([[10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24],\n",
      "        [25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34]])\n"
     ]
    }
   ],
   "source": [
    "# convert back to image 1 Color channel\n",
    "\n",
    "import torch\n",
    "\n",
    "IMAGE_SIZE = 5\n",
    "\n",
    "\n",
    "spiral_indices = torch.tensor(create_spiral(IMAGE_SIZE)).flatten()\n",
    "\n",
    "# Erstellen des Beispiel-Daten-Tensors und des spiralförmigen Index-Tensors (erneut)\n",
    "data_tensor = torch.arange(IMAGE_SIZE*IMAGE_SIZE).reshape(IMAGE_SIZE, IMAGE_SIZE) + 10\n",
    "\n",
    "# Erstellen des umgeordneten Tensors\n",
    "correct_reordered_tensor = torch.zeros_like(data_tensor.flatten())\n",
    "correct_reordered_tensor[spiral_indices] = data_tensor.flatten()\n",
    "\n",
    "\n",
    "positions_in_spiral = torch.argsort(spiral_indices)\n",
    "\n",
    "reconstructed_tensor_efficient = torch.zeros_like(data_tensor).flatten()\n",
    "reconstructed_tensor_efficient[positions_in_spiral] = correct_reordered_tensor\n",
    "\n",
    "# Rückumwandlung in 8x8 Format\n",
    "reconstructed_tensor_efficient = reconstructed_tensor_efficient.view(IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "print(data_tensor)\n",
    "print(reconstructed_tensor_efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAnCAYAAACSeUteAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABkklEQVR4nO3cL0tfYRzG4cc/zCIYfAFiFYNBBEVhacEmbKAIWiyCTRBMFjEsD0TYsqDRICwpgsEgCCZFkwuzCL6Bs5fgwhd+4b6u/HCfJ5zw4YTT13Vd1wCAWP29vgAA0FtiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAINzg/x7cnh0tffDk6U7p3sGfq7Ktp4Gzsq3WWps+mS/dW377XLo3PnxXuvf85bx0b2Lwb+ne3Mtr2dbJ0X7ZVmutbS7tle5tPDyW7h3/Hirdu7lYKd2beKj7h9rYSO17t/t1u3Rvbfe2dG9qcat0b+Z9tXTv56cfZVvX37+VbbXW2vr9Qune5cyv0r2xw9kPz/gyAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEK6v67qu15cAAHrHlwEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAj3D2SELkdNhj7bAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4502, 0.2224, 0.0728],\n",
      "         [0.6200, 0.8935, 0.5371],\n",
      "         [0.1448, 0.7849, 0.2962],\n",
      "         [0.0057, 0.7991, 0.9980],\n",
      "         [0.2098, 0.4625, 0.2295],\n",
      "         [0.5562, 0.3935, 0.4788],\n",
      "         [0.6858, 0.4411, 0.2950],\n",
      "         [0.5518, 0.7142, 0.0103],\n",
      "         [0.7151, 0.7307, 0.9242],\n",
      "         [0.9439, 0.6220, 0.8364],\n",
      "         [0.6068, 0.2188, 0.3491],\n",
      "         [0.0126, 0.5361, 0.8499],\n",
      "         [0.4074, 0.3881, 0.7023],\n",
      "         [0.0557, 0.1105, 0.7304],\n",
      "         [0.8495, 0.8661, 0.0771],\n",
      "         [0.0110, 0.7158, 0.0706],\n",
      "         [0.1302, 0.7743, 0.9844],\n",
      "         [0.6070, 0.1009, 0.4340],\n",
      "         [0.9740, 0.5744, 0.2387],\n",
      "         [0.1587, 0.8646, 0.6597],\n",
      "         [0.3720, 0.8108, 0.0133],\n",
      "         [0.9807, 0.8316, 0.5927],\n",
      "         [0.7539, 0.3691, 0.9257],\n",
      "         [0.1221, 0.1987, 0.1644],\n",
      "         [0.8792, 0.4100, 0.7784]]])\n",
      "tensor([[[0.8792, 0.1221, 0.7539, 0.9807, 0.3720],\n",
      "         [0.9439, 0.7151, 0.5518, 0.6858, 0.1587],\n",
      "         [0.6068, 0.6200, 0.4502, 0.5562, 0.9740],\n",
      "         [0.0126, 0.1448, 0.0057, 0.2098, 0.6070],\n",
      "         [0.4074, 0.0557, 0.8495, 0.0110, 0.1302]],\n",
      "\n",
      "        [[0.4100, 0.1987, 0.3691, 0.8316, 0.8108],\n",
      "         [0.6220, 0.7307, 0.7142, 0.4411, 0.8646],\n",
      "         [0.2188, 0.8935, 0.2224, 0.3935, 0.5744],\n",
      "         [0.5361, 0.7849, 0.7991, 0.4625, 0.1009],\n",
      "         [0.3881, 0.1105, 0.8661, 0.7158, 0.7743]],\n",
      "\n",
      "        [[0.7784, 0.1644, 0.9257, 0.5927, 0.0133],\n",
      "         [0.8364, 0.9242, 0.0103, 0.2950, 0.6597],\n",
      "         [0.3491, 0.5371, 0.0728, 0.4788, 0.2387],\n",
      "         [0.8499, 0.2962, 0.9980, 0.2295, 0.4340],\n",
      "         [0.7023, 0.7304, 0.0771, 0.0706, 0.9844]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert back to image 3 colors\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import torch\n",
    "\n",
    "IMAGE_SIZE = 5\n",
    "\n",
    "\n",
    "spiral_indices = torch.tensor(create_spiral(IMAGE_SIZE)).flatten()\n",
    "\n",
    "# Erstellen des Beispiel-Daten-Tensors und des spiralförmigen Index-Tensors (erneut)\n",
    "data_tensor = torch.rand((1,IMAGE_SIZE*IMAGE_SIZE,3))\n",
    "\n",
    "positions_in_spiral = torch.argsort(spiral_indices)\n",
    "\n",
    "reconstructed_tensor_efficient = torch.zeros((3,IMAGE_SIZE*IMAGE_SIZE))\n",
    "reconstructed_tensor_efficient[:,positions_in_spiral] = rearrange(data_tensor, '1 h c -> c h')\n",
    "\n",
    "# Rückumwandlung in 8x8 Format\n",
    "reconstructed_tensor_efficient = reconstructed_tensor_efficient.view(3,IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "plt.imshow(data_tensor)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(data_tensor)\n",
    "print(reconstructed_tensor_efficient)\n",
    "\n",
    "plt.imshow(rearrange(reconstructed_tensor_efficient, 'c h w -> h w c'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAaCAYAAAAqnV8tAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAA00lEQVR4nO3cUQrCMBBF0Ubd/4or4waiFhka9Z3zGQsTmJ9LKY6qqg0AiHVZfQEAYC0xAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEC42+Enx5ge79t1en5vOn/12/7k+l2zu+b23mk+2x7eP997J3v4dO7K2fbwHbPP2M+v7KF3P/PZR/5n2JsBAAgnBgAgnBgAgHBiAADCiQEACDeqjnxnCAD8K28GACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAg3AMg3FotfCH8hwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create expample image to explain the spiral matrix concept\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "size = 9\n",
    "\n",
    "\n",
    "def generate_color_gradient(length):\n",
    "    # Generate a gradient for each color channel\n",
    "    r = np.linspace(1, 0, length)\n",
    "    g = np.linspace(0, 0, length)  # Green channel remains constant at 0\n",
    "    b = np.linspace(0, 1, length)  # Blue channel increases from 0 to 1\n",
    "\n",
    "    # Stack the color channels to form the gradient\n",
    "    gradient = np.stack((r, g, b), axis=1)\n",
    "\n",
    "    return gradient\n",
    "\n",
    "\n",
    "spiral_indices = torch.tensor(create_spiral(size)).flatten()\n",
    "\n",
    "gradient_tensor = generate_color_gradient(size*size)\n",
    "\n",
    "gradient_tensor_reshaped = gradient_tensor[np.newaxis, :, :]\n",
    "\n",
    "# Plotting the color gradient\n",
    "plt.imshow(gradient_tensor_reshaped)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "gradient_tensor_reshaped_tensor = torch.tensor(gradient_tensor_reshaped).float()\n",
    "positions_in_spiral = torch.argsort(spiral_indices)\n",
    "\n",
    "reconstructed_tensor_efficient = torch.zeros((3,size*size))\n",
    "reconstructed_tensor_efficient[:,positions_in_spiral] = rearrange(gradient_tensor_reshaped_tensor, '1 h c -> c h')\n",
    "\n",
    "# Rückumwandlung in 8x8 Format\n",
    "reconstructed_tensor_efficient = reconstructed_tensor_efficient.view(3,size, size)\n",
    "\n",
    "\n",
    "plt.imshow(rearrange(reconstructed_tensor_efficient, 'c h w -> h w c'))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
